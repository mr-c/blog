.. post:: 
   :tags: weekly-update
   :author: me
   :location: Austin

********************************************
Summary of TACC Life Science Computing visit
********************************************

Met with John Fonner, Joe Stubbs, Matt Vaghen, Rion Dooley, Victor Eijkhout

Very positive about CWL; they have agreed to become a CWL partner; effort will
come from TACC sources; possibly also IPlant. They are also working on an app
directory.

Existing capabilities:

From the CWL perspective, `The Agave Platform <http://agaveapi.co/>`__ is a
multi-tenent, multi-execution-environment remote job runner. The primary use
case is submission of jobs via a command line tool; specification of tool
options is done via a JSON formated plain text file.

Their workflow manager is called `endofday`. It started as a nextflow based
docker orchestration program; it is now a pydoit based Docker & Agave
application orchestration program. For long-running analysis steps; not (web)
services.

Areas of concern:

How and where to make link between a generic CWL tool description & a
particular tenant? This will be a concern for other platforms that don't use
Docker, such as Galaxy.

Their asks:
[1] site specific config
[2] Python & Java SDKs/libraries autogenerated from the spec for parsing CWL
files.
[3] document how to run the test suite by hand
[4] best practices document: imports at top; IDs defined explicitly for each
tool.
[5] reduce syntax verbosity via implicit namespacing. [Does Draft 3 satisfy
that?]


Follow up: John Fonner & others to present Agave & their workflow system to the
CWL group during the December 1st video chat. They will meet privately after
that to organize; MRC to follow up on Dec

A lot of the discussion was about the collaboration model between the larger
CWL community and specific implementations: how will tool and workflow
descriptions be shared?

For implementations not using Docker: one collaboration model is to fork each
tool description as that tool is installed: adding implementation specific
fields to indicate which tenant the tool is installed to and other required
details. In the case of a tool being installed multiple times the tool ID would
be changed to allow for unique references from workflows. In this model
workflows from outside sources would also be customized to refer to these
platform-specific tools.

Concerns about the portability of such workflows outside the implementations
that produced them were raised. 

Another proposal was to add another stanza to the job document (along with the
already approved for Draft 3 identifier of which workflow or tool to run).

However this could get quite unwieldy for users, especially for complex
workflows with many steps & applications.

While this information could be added on a per-tool basis to the CLI interface
description document it would require changing the tool IDs from the community
maintained copies thus breaking portability of workflows that reference such
tools.

Misc questions:

How to mark input as required / optional? (Is this the `type: [null, ...]`
trick?
Would like to be able to feed output document back in as new input document to
reproduce/re-do analysis automatically. Great idea, easily doable by adding the
input document to the output object and updating the spec to specify that the
output stanza (if any) should be ignored on input objects.
